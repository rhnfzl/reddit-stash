name: Claude Code Review

on:
  pull_request:
    types: [opened, synchronize]
    # Only run on Python and configuration file changes
    paths:
      - "**/*.py"
      - "requirements*.txt"
      - "settings.ini"
      - "Dockerfile"
      - ".github/workflows/*.yml"

jobs:
  claude-review:
    # Optional: Filter by PR author
    # if: |
    #   github.event.pull_request.user.login == 'external-contributor' ||
    #   github.event.pull_request.user.login == 'new-developer' ||
    #   github.event.pull_request.author_association == 'FIRST_TIME_CONTRIBUTOR'
    
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude Code Review
        id: claude-review
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          prompt: |
            REPO: ${{ github.repository }}
            PR NUMBER: ${{ github.event.pull_request.number }}

            You are reviewing a Python application for backing up Reddit content (saved posts, comments, user activity) with advanced media download and content recovery systems.

            ## Project Context
            - Python 3.10+ application using PRAW for Reddit API integration
            - 4-provider content recovery cascade (Wayback Machine, PullPush.io, Reddit Previews, Reveddit)
            - Complex media download system with service-specific handlers (Imgur, Reddit, generic HTTP)
            - SQLite databases for caching and retry queue management
            - Dropbox integration for cloud backup
            - Comprehensive rate limiting with token bucket implementations

            ## Review Focus Areas

            ### 1. Python Best Practices
            - Python 3.10+ compatibility and proper type hints
            - PRAW API usage patterns and error handling
            - Proper use of context managers and resource cleanup
            - Dependencies compatibility (check requirements.txt)

            ### 2. Security Review
            - **Credential Safety**: Check for hardcoded secrets, API keys, tokens
            - **Path Traversal**: Validate utils/path_security.py usage for file operations
            - **URL Validation**: Check utils/url_security.py implementation for download URLs
            - **SQL Injection**: Review SQLite query construction in cache_manager.py
            - **Input Sanitization**: Verify all user inputs and external data

            ### 3. API Integration & Rate Limiting
            - Reddit API rate limit compliance (100 req/min via PRAW)
            - Service-specific rate limits (Imgur: 4 req/min, PullPush: 12 req/min)
            - Token bucket implementations in utils/media_services/
            - Retry-After header respect for 429 responses
            - Exponential backoff patterns for failures

            ### 4. Media Download System
            - URL preprocessing (utils/url_transformer.py) for direct downloads
            - Recovery cascade implementation (utils/content_recovery/)
            - Session-level deduplication to prevent duplicate downloads
            - File integrity checks with BLAKE3 hashing
            - Proper cleanup of temporary files (utils/temp_file_utils.py)

            ### 5. Performance Considerations
            - SQLite query optimization and indexing strategies
            - Database connection pooling and WAL mode usage
            - Memory usage for large GDPR CSV processing
            - Disk space validation before downloads (20% safety margin)
            - Efficient batch processing for multiple media files

            ### 6. Error Handling
            - Graceful handling of deleted/404 content
            - Network timeout and retry strategies
            - Disk full and permission error handling
            - API failure recovery patterns
            - Proper logging without exposing sensitive data

            ### 7. Test Coverage
            - Content recovery system tests (tests/run_recovery_tests.py)
            - Media download edge cases (double extensions, HTML wrappers)
            - Rate limiting behavior under load
            - Database operation atomicity
            - Cleanup script effectiveness

            ## Critical Architecture & Security Patterns

            ### Key File Structure
            - `reddit_stash.py`: Main entry point
            - `utils/media_download_manager.py`: Central media coordinator
            - `utils/content_recovery/recovery_service.py`: 4-provider cascade orchestration
            - `utils/path_security.py`: Path traversal prevention (MUST use for file operations)
            - `utils/url_security.py`: URL validation (MUST use for downloads)
            - `utils/cache_manager.py`: SQLite operations with WAL mode
            - `utils/temp_file_utils.py`: Guaranteed temp file cleanup

            ### Rate Limiting Implementation
            - **Reddit API**: 100 req/min via PRAW (built-in)
            - **Imgur**: 4 req/min (conservative, 240/hour with safety buffer)
            - **PullPush.io**: 12 req/min (under 15 soft limit)
            - **Wayback Machine**: 60 req/min
            - **Generic HTTP**: 30 req/min
            - Token bucket pattern with jitter: ±10% (Reddit) / ±25% (HTTP)
            - ALWAYS respect Retry-After headers for 429 responses

            ### Security Requirements
            - Path operations MUST use `utils/path_security.py` sanitization
            - Download URLs MUST use `utils/url_security.py` validation
            - SQLite queries MUST use parameterized statements
            - NO hardcoded credentials (check for REDDIT_*, DROPBOX_*, IMGUR_* secrets)
            - Temp files MUST use context managers from `utils/temp_file_utils.py`

            ### Recent Critical Fixes (Check for regressions)
            - **Reddit Image HTML Issue**: Fixed Accept headers to prioritize image formats
            - **PyImgur Double Extensions**: Strip extensions before PyImgur processing
            - **URL Preprocessing**: GitHub/GitLab blob URLs → raw download URLs
            - **Session Deduplication**: URL tracking prevents duplicate downloads
            - **File Integrity**: BLAKE3 hashing for download verification

            ## Review Output
            Provide specific, actionable feedback focusing on:
            1. Critical security vulnerabilities (MUST fix)
            2. Performance bottlenecks that could impact production
            3. API compliance issues that could cause rate limiting or bans
            4. Code maintainability improvements
            5. Missing error handling for edge cases

            Be constructive and suggest specific code improvements where applicable.

            Use `gh pr comment` with your Bash tool to leave your detailed review as a comment on the PR.
          
          # See https://github.com/anthropics/claude-code-action/blob/main/docs/usage.md
          # or https://docs.claude.com/en/docs/claude-code/sdk#command-line for available options
          claude_args: '--allowed-tools "Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*),Bash(python -m py_compile:*),Bash(python -c:*),Bash(ruff check:*)"'

